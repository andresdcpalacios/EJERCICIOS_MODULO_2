{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. (2 punto) Genera una base de datos con SQL Lite. En lugar de SQL Lite puedes utilizar"
      ],
      "metadata": {
        "id": "HN9YRI3GPRao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jLlfM464PVF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('big_bang_theory.db')\n",
        "print(\"Base de datos creada y conectada con éxito\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znyMVFLdPX5A",
        "outputId": "cff0aaf1-7000-4a93-987d-898156cb770f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base de datos creada y conectada con éxito\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 punto Conecta a esa base de datos con pyodbc o SQL Alchemy. Crea una tabla en la base de datos desde un data frame de pandas creado a partir de los datos del fichero big_bang_theory_dataset.csv que hay en el aula virtual."
      ],
      "metadata": {
        "id": "w0AG-P1XkwqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Construct the absolute path to the CSV file\n",
        "csv_file_path = os.path.join(current_dir, 'big_bang_theory_dataset.csv')\n",
        "\n",
        "# Cargar el dataset en un DataFrame using the absolute path\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Crear la conexión a la base de datos con SQLAlchemy\n",
        "engine = create_engine('sqlite:///big_bang_theory.db')\n",
        "\n",
        "# Crear la tabla en la base de datos\n",
        "df.to_sql('big_bang_theory', con=engine, if_exists='replace', index=False)\n",
        "print(\"Tabla creada en la base de datos con éxito.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "4LJPWJgZk4aX",
        "outputId": "88af6a0d-5642-4c37-e3f7-ad0d51e2b8d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/big_bang_theory_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2a39ffe8cd1b>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Cargar el dataset en un DataFrame using the absolute path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Crear la conexión a la base de datos con SQLAlchemy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/big_bang_theory_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Realiza un histograma con las localizaciones donde Sheldon dice exclusivamente “Penny.”. Representa junto al anterior histograma otro con las localizaciones donde Sheldon dice algún texto que contenga “Penny” (podrías utilizar el método de pandas Series.str.contains)."
      ],
      "metadata": {
        "id": "fCITayKznDY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filtrar los diálogos donde Sheldon dice exclusivamente \"Penny.\"\n",
        "df_penny_exact = df[(df['Personaje'] == 'Sheldon') & (df['Texto'] == \"Penny.\")]\n",
        "# Filtrar los diálogos donde Sheldon menciona \"Penny\" en cualquier parte del texto\n",
        "df_penny_contains = df[(df['Personaje'] == 'Sheldon') & (df['Texto'].str.contains(\"Penny\", case=False))]\n",
        "\n",
        "# Crear histogramas\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "df_penny_exact['Localización'].value_counts().plot(kind='bar', color='blue')\n",
        "plt.title(\"Localizaciones donde Sheldon dice exclusivamente 'Penny.'\")\n",
        "plt.xlabel(\"Localización\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df_penny_contains['Localización'].value_counts().plot(kind='bar', color='orange')\n",
        "plt.title(\"Localizaciones donde Sheldon menciona 'Penny'\")\n",
        "plt.xlabel(\"Localización\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T4fyKkdZnI00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Usando matplotlib. Representa las localizaciones de la escena en el eje Y y el personaje que habla en el eje X."
      ],
      "metadata": {
        "id": "uXAXXwvPnNIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Representar localizaciones en el eje Y y personajes en el eje X\n",
        "plt.scatter(df['Personaje'], df['Localización'], alpha=0.5)\n",
        "plt.title(\"Localizaciones de las escenas vs Personaje que habla\")\n",
        "plt.xlabel(\"Personaje\")\n",
        "plt.ylabel(\"Localización\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KAezmSiDrU8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Agrupa el conjunto de datos por la temporada y el personaje. Para cada grupo genera una columna con el número de veces que se pronuncia la palabra “Penny” y otra columna con un índice para el grupo creado por personaje y temporada (podrías utilizar el método de pandas ngroup). Usando sklearn, realiza una regresión logística que relacione el número de veces que se dice “Penny” en una temporada con el personaje que lo dice\n"
      ],
      "metadata": {
        "id": "yIzE51IQrYbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Contar el número de veces que cada personaje menciona \"Penny\" por temporada\n",
        "df['Penny_count'] = df['Texto'].str.contains(\"Penny\", case=False).astype(int)\n",
        "grouped = df.groupby(['Temporada', 'Personaje']).agg(\n",
        "    Penny_count=('Penny_count', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# Crear índice para cada grupo por personaje y temporada\n",
        "grouped['Personaje_index'] = grouped.groupby('Personaje').ngroup()\n",
        "\n",
        "# Preparar los datos para la regresión logística\n",
        "X = grouped[['Penny_count']]\n",
        "y = grouped['Personaje_index']\n",
        "\n",
        "# Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear y entrenar el modelo\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones y evaluar el modelo\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "NxBZlE1orgYI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}